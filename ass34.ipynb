{
 "cells": [
  {
   "cell_type": "raw",
   "id": "337d2730-fa10-430f-88d9-60bd04d93f1d",
   "metadata": {},
   "source": [
    "#Q1.\n",
    "\n",
    "Analysis of Variance (ANOVA) is a statistical technique used to compare means among different groups and determine if there are significant differences among those means. ANOVA makes certain assumptions about the data in order to provide accurate and reliable results. Violations of these assumptions can impact the validity of ANOVA results. The main assumptions of ANOVA are:\n",
    "\n",
    "    Independence: The observations within each group must be independent of each other. This means that the values in one group should not influence or be influenced by the values in another group.\n",
    "\n",
    "    Normality: The residuals (the differences between the observed values and the group means) should be approximately normally distributed within each group. This assumption is important because ANOVA uses the normal distribution to calculate p-values and confidence intervals.\n",
    "\n",
    "    Homogeneity of Variance (Homoscedasticity): The variability (variance) of the residuals should be relatively constant across all groups. In other words, the spread of the data points around the group means should be similar for all groups.\n",
    "\n",
    "If any of these assumptions are violated, the validity of ANOVA results can be compromised. Here are examples of violations for each assumption:\n",
    "\n",
    "    Independence:\n",
    "    Violation Example: In a study comparing the effectiveness of different teaching methods on students' test scores, if students within the same classroom are assigned to different teaching methods, their scores may be correlated due to classroom-level factors, violating the assumption of independence.\n",
    "\n",
    "    Normality:\n",
    "    Violation Example: Consider a study comparing reaction times of participants exposed to different stimuli. If the reaction times are skewed or have heavy tails in one or more groups, it can violate the normality assumption.\n",
    "\n",
    "    Homogeneity of Variance:\n",
    "    Violation Example: Suppose you're comparing the yields of three different fertilizers on plants. If the variability in yields is much larger for one fertilizer compared to the others, this violates the assumption of homogeneity of variance.\n",
    "\n",
    "When these assumptions are violated, it's important to consider alternative analysis methods or transformations of the data to address the issues. For example:\n",
    "\n",
    "    If the assumption of normality is violated, you might consider using non-parametric tests (e.g., Kruskal-Wallis test) or transforming the data to achieve approximate normality (e.g., logarithmic transformation).\n",
    "    If the assumption of homogeneity of variance is violated, you might consider using Welch's ANOVA (which is more robust to unequal variances) or transforming the data.\n",
    "\n",
    "Additionally, if your data violate multiple assumptions and transformations don't help, you might explore more advanced techniques like generalized linear models or mixed-effects models, which can handle more complex data structures and relax some of the assumptions.\n",
    "\n",
    "In summary, understanding and checking the assumptions of ANOVA are crucial to ensure the validity and reliability of the results. If assumptions are violated, appropriate remedies should be considered to obtain accurate statistical conclusions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ba496fc-f2b6-4eae-a876-843fcccf8905",
   "metadata": {},
   "source": [
    "#Q2.\n",
    "\n",
    "\n",
    "Analysis of Variance (ANOVA) is a statistical method used to analyze the differences between group means in a data set. There are three main types of ANOVA: One-Way ANOVA, Two-Way ANOVA, and Multivariate ANOVA (MANOVA). Each type is used in different situations:\n",
    "\n",
    "    One-Way ANOVA:\n",
    "    One-Way ANOVA is used when you have one independent variable (factor) with more than two levels (groups) and you want to determine if there are significant differences in the means of the dependent variable across these groups. For example, you might use One-Way ANOVA to compare the average test scores of students from different schools or to analyze the effects of different treatments on a single outcome variable.\n",
    "\n",
    "    Two-Way ANOVA:\n",
    "    Two-Way ANOVA is an extension of One-Way ANOVA and is used when you have two independent variables (factors) and you want to examine their combined effects on a dependent variable. It allows you to determine whether there are significant main effects for each factor and whether there is an interaction effect between the two factors. For example, you might use Two-Way ANOVA to analyze the effects of both gender and different teaching methods on student performance.\n",
    "\n",
    "    Multivariate ANOVA (MANOVA):\n",
    "    MANOVA is used when you have multiple dependent variables and one or more independent variables. It's an extension of ANOVA that takes into account correlations among the dependent variables. MANOVA allows you to determine whether there are significant differences in the combined set of dependent variables across different groups defined by the independent variables. This is useful when you want to investigate relationships between multiple variables simultaneously. For instance, you might use MANOVA to analyze the effects of different treatments on multiple related health outcomes.\n",
    "\n",
    "In summary:\n",
    "\n",
    "    One-Way ANOVA: One independent variable, multiple groups. Used to compare means across different groups.\n",
    "    Two-Way ANOVA: Two independent variables, multiple groups. Used to examine main effects and interaction effects of two factors on a dependent variable.\n",
    "    Multivariate ANOVA (MANOVA): One or more independent variables, multiple dependent variables. Used to analyze the combined effects of independent variables on multiple dependent variables.\n",
    "\n",
    "The choice of which type of ANOVA to use depends on the nature of your data, research questions, and experimental design."
   ]
  },
  {
   "cell_type": "raw",
   "id": "df5872ee-91ab-443f-8efa-7d830f226438",
   "metadata": {},
   "source": [
    "#Q3.\n",
    "\n",
    "\n",
    "The partitioning of variance in Analysis of Variance (ANOVA) refers to the process of breaking down the total variability observed in a dataset into different components, each of which is attributed to a specific source or factor. ANOVA is a statistical method used to compare means across multiple groups or conditions and determine whether any of those groups differ significantly from each other. The partitioning of variance helps us understand how much of the variability in the dependent variable can be explained by the independent variables (factors) in the study.\n",
    "\n",
    "In the context of a one-way ANOVA (which deals with one categorical independent variable), the total variance in the dependent variable is divided into two main components:\n",
    "\n",
    "    Between-group variance (explained variance): This variance is due to the differences between the means of the groups being compared. It represents the variability that can be attributed to the effect of the independent variable (factor). The greater the between-group variance, the more likely it is that the independent variable has a significant effect on the dependent variable.\n",
    "\n",
    "    Within-group variance (unexplained variance or error variance): This variance is due to the random variability within each group. It represents the variability that cannot be attributed to the effect of the independent variable and is often considered as random noise or measurement error. It includes individual differences, measurement errors, and other factors not accounted for in the analysis.\n",
    "\n",
    "The formula for calculating the total variance is:\n",
    "\n",
    "Total Variance = Between-Group Variance + Within-Group Variance\n",
    "\n",
    "Mathematically, the partitioning of variance is crucial for determining whether the observed differences between group means are statistically significant or if they could have occurred by chance. The F-ratio (F-statistic) is calculated by comparing the between-group variance to the within-group variance. If the between-group variance is significantly larger than the within-group variance, it suggests that the groups are not equal, and the null hypothesis of no group differences can be rejected.\n",
    "\n",
    "Understanding the concept of partitioning of variance is important for several reasons:\n",
    "\n",
    "    Interpretation of Results: By understanding how much of the total variability is attributed to the independent variable, researchers can interpret the strength of the relationship or effect.\n",
    "\n",
    "    Effect Size: The partitioning of variance can help quantify the effect size, which is the magnitude of the difference between groups. This is important for assessing the practical significance of the results.\n",
    "\n",
    "    Model Evaluation: It allows researchers to assess the goodness-of-fit of the model and the adequacy of the independent variable(s) in explaining the variability observed in the dependent variable.\n",
    "\n",
    "    Decision Making: Understanding the partitioning of variance assists in making informed decisions about the significance of group differences and whether further investigation or experimentation is warranted.\n",
    "\n",
    "In summary, the partitioning of variance in ANOVA is a fundamental concept that helps researchers assess the impact of independent variables on dependent variables, understand the significance of group differences, and make meaningful interpretations based on statistical analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdf19f2-b6be-490d-8efb-75d85cf663c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST: 1120.0\n",
      "SSE: 120.0\n",
      "SSR: 1000.0\n",
      "F-statistic: 50.0\n",
      "p-value: 1.5127924217375409e-06\n"
     ]
    }
   ],
   "source": [
    "#Q4.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Generate example data for different groups\n",
    "group1 = np.array([22, 24, 26, 28, 30])\n",
    "group2 = np.array([32, 34, 36, 38, 40])\n",
    "group3 = np.array([42, 44, 46, 48, 50])\n",
    "\n",
    "# Combine all data into a single array\n",
    "all_data = np.concatenate((group1, group2, group3))\n",
    "\n",
    "# Calculate the overall mean\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate the sum of squares for the total variability (SST)\n",
    "sst = np.sum((all_data - overall_mean) ** 2)\n",
    "\n",
    "# Calculate the sum of squares for each group's variability (SSE)\n",
    "sse_group1 = np.sum((group1 - np.mean(group1)) ** 2)\n",
    "sse_group2 = np.sum((group2 - np.mean(group2)) ** 2)\n",
    "sse_group3 = np.sum((group3 - np.mean(group3)) ** 2)\n",
    "sse = sse_group1 + sse_group2 + sse_group3\n",
    "\n",
    "# Calculate the residual sum of squares (SSR)\n",
    "ssr = sst - sse\n",
    "\n",
    "# Perform one-way ANOVA using scipy\n",
    "f_statistic, p_value = f_oneway(group1, group2, group3)\n",
    "\n",
    "print(\"SST:\", sst)\n",
    "print(\"SSE:\", sse)\n",
    "print(\"SSR:\", ssr)\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af4133-98be-42eb-a004-3c16a43fa2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5.\n",
    "\n",
    "\n",
    "#n a two-way ANOVA (Analysis of Variance), you analyze the effects of two categorical independent variables (factors) on a continuous dependent variable. The main effects refer to the individual effects of each factor, while the interaction effect refers to the combined effect of both factors on the dependent variable.\n",
    "\n",
    "#You can perform a two-way ANOVA and calculate the main and interaction effects using Python with the help of libraries such as scipy and statsmodels. Here's a step-by-step guide:\n",
    "\n",
    "   # Import Required Libraries:\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "   # Prepare Your Data:\n",
    "   # Assuming you have a DataFrame with your data where columns represent the two factors (categorical variables) and the dependent variable (continuous variable), you can load your data into a pandas DataFrame.\n",
    "\n",
    "\n",
    "\n",
    "# Create or load your DataFrame\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "    #Perform Two-Way ANOVA:\n",
    "    #You can perform the two-way ANOVA using scipy.stats.f_oneway to calculate the main effects and interaction effect.\n",
    "\n",
    "\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "result = f_oneway(data['Dependent_Variable'], data['Factor1'], data['Factor2'], data['Factor1_Factor2'])\n",
    "\n",
    "# Print the results\n",
    "print(\"Overall F-statistic:\", result.statistic)\n",
    "print(\"P-value:\", result.pvalue)\n",
    "\n",
    "    #Perform Two-Way ANOVA with Interaction Using statsmodels:\n",
    "    #For a more detailed analysis and to obtain the main effects and interaction effects, you can use the statsmodels library.\n",
    "\n",
    "\n",
    "\n",
    "# Fit the two-way ANOVA model with interaction\n",
    "model = ols('Dependent_Variable ~ Factor1 * Factor2', data=data).fit()\n",
    "\n",
    "# Perform ANOVA analysis\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table)\n",
    "\n",
    "#In the ANOVA table, you will find columns labeled with \"PR(>F)\" representing the p-values for the main effects and interaction effect. If the p-value is significant (typically below 0.05), it indicates that the corresponding factor or interaction has a significant effect on the dependent variable.\n",
    "\n",
    "#Remember to replace 'Dependent_Variable', 'Factor1', 'Factor2', and 'Factor1_Factor2' with the actual column names in your DataFrame."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f124bfe4-b26b-43fb-b679-e8f44977ca2b",
   "metadata": {},
   "source": [
    "#Q6.\n",
    "\n",
    "\n",
    "In a one-way Analysis of Variance (ANOVA), the F-statistic is used to test the null hypothesis that all group means are equal. The p-value associated with the F-statistic helps determine whether this null hypothesis can be rejected.\n",
    "\n",
    "In your case, you obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "\n",
    "    Interpretation of the F-statistic:\n",
    "    The F-statistic is a ratio of variance between groups to variance within groups. A higher F-statistic indicates that the variation between group means is relatively larger compared to the variation within each group. In your case, the F-statistic of 5.23 suggests that there might be some differences between the group means.\n",
    "\n",
    "    Interpretation of the p-value:\n",
    "    The p-value associated with the F-statistic is used to determine the statistical significance of the differences between the groups. A p-value of 0.02 means that if the null hypothesis (all group means are equal) were true, there is only a 2% chance of obtaining an F-statistic as extreme as the one you observed, purely due to random chance.\n",
    "\n",
    "    Conclusion:\n",
    "    Since the p-value (0.02) is less than the commonly used significance level of 0.05, you have evidence to reject the null hypothesis. This suggests that there are statistically significant differences between at least two of the groups. However, the ANOVA itself doesn't tell you which specific groups are different from each other; it only tells you that at least one pair of groups has statistically significant differences in their means.\n",
    "\n",
    "    Further Analysis:\n",
    "    If your ANOVA indicates significant differences, you might want to perform post hoc tests (e.g., Tukey's HSD, Bonferroni) to identify which specific groups differ from each other. These tests would provide more detailed information about the pairwise differences and help you pinpoint which group means are significantly different.\n",
    "\n",
    "Remember that while statistical significance suggests that differences exist, it doesn't necessarily imply that the differences are practically significant or meaningful. Practical significance depends on the context of your study and the subject matter.\n",
    "\n",
    "In summary, based on an F-statistic of 5.23 and a p-value of 0.02, you can conclude that there are statistically significant differences between at least some of the groups you analyzed."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab7bffb5-144a-4143-b008-c23109f8291f",
   "metadata": {},
   "source": [
    "#Q7.\n",
    "\n",
    "\n",
    "Handling missing data in a repeated measures ANOVA is crucial to ensure the validity and reliability of your statistical analysis. There are several methods you can use to handle missing data, each with its own advantages and potential consequences. Here are some common methods and their potential consequences:\n",
    "\n",
    "    Listwise Deletion (Complete Case Analysis): This involves excluding cases with any missing values from the analysis. While it's simple, it can lead to a reduction in sample size, potential bias if missingness is not completely at random (MCAR), and reduced statistical power. It can also introduce bias if the missing data mechanism is related to the variables being studied.\n",
    "\n",
    "    Pairwise Deletion: In this approach, you analyze all available data for each pair of variables, ignoring cases with missing values only in those specific variables. Like listwise deletion, this can lead to reduced power and potential bias.\n",
    "\n",
    "    Imputation Methods:\n",
    "        Mean/Median Imputation: Replace missing values with the mean or median of the observed values for that variable. This can introduce bias if the missing data mechanism is not MCAR and may underestimate the standard errors.\n",
    "        Regression Imputation: Use regression equations to predict missing values based on other observed variables. This can lead to biased estimates if the regression model is not accurate.\n",
    "        Multiple Imputation: Generate multiple plausible imputed datasets and analyze them separately, then combine the results. This method accounts for the uncertainty introduced by imputation and provides more accurate parameter estimates and standard errors. However, it can be computationally intensive.\n",
    "\n",
    "    Maximum Likelihood Estimation (MLE): This method estimates model parameters by maximizing the likelihood function, accounting for missing data patterns. It's considered a robust approach, especially under the assumption of missing at random (MAR), but it can be complex to implement and may require specialized software.\n",
    "\n",
    "    Expectation-Maximization (EM) Algorithm: This iterative algorithm estimates missing data by repeatedly updating parameter estimates and imputations. It's especially useful for complex models, but convergence issues can occur, and it may require careful handling.\n",
    "\n",
    "The potential consequences of using different methods to handle missing data include:\n",
    "\n",
    "    Bias: Using improper methods can introduce bias in parameter estimates, leading to incorrect conclusions about the relationships in the data.\n",
    "    Reduced Statistical Power: Deletion methods and imputation methods that do not properly account for uncertainty can lead to reduced statistical power and increased Type II error rates.\n",
    "    Inaccurate Confidence Intervals and p-values: Incorrect handling of missing data can lead to incorrect standard errors, confidence intervals, and p-values, affecting the reliability of your statistical inferences.\n",
    "    Invalid Results: If missing data is not handled appropriately, your results may not accurately reflect the underlying population, potentially rendering your analysis invalid.\n",
    "    Misleading Interpretations: Inaccurate handling of missing data can lead to incorrect interpretations and recommendations based on the analysis.\n",
    "\n",
    "When handling missing data in a repeated measures ANOVA, it's important to consider the missing data mechanism, the characteristics of your data, and the assumptions of the analysis method you are using. Generally, multiple imputation and maximum likelihood estimation are recommended for handling missing data, as they provide more accurate and robust results compared to simple imputation or deletion methods. However, the choice of method also depends on the specific circumstances of your data and analysis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "014554a9-d88e-4306-9b5a-c68238b4f2ad",
   "metadata": {},
   "source": [
    "#Q8.\n",
    "\n",
    "\n",
    "After conducting an Analysis of Variance (ANOVA) and finding a significant difference among the group means, post-hoc tests are often used to determine which specific group means are significantly different from each other. Post-hoc tests help to identify pairwise comparisons that contribute to the significant overall ANOVA result. Here are some common post-hoc tests and when to use them:\n",
    "\n",
    "    Tukey's Honestly Significant Difference (HSD):\n",
    "    Tukey's HSD test is conservative and controls the familywise error rate. It's appropriate when you have a balanced design (equal sample sizes in all groups) and you want to make all possible pairwise comparisons. It's commonly used when you don't have specific hypotheses about which groups differ from each other.\n",
    "\n",
    "    Bonferroni Correction:\n",
    "    The Bonferroni correction divides the desired significance level (usually 0.05) by the number of pairwise comparisons. It's useful when you have a limited number of specific hypotheses to test and you want to maintain a stricter control over the familywise error rate. However, it can be conservative and increase the chance of Type II errors.\n",
    "\n",
    "    Scheffe's Test:\n",
    "    Scheffe's test is appropriate for complex designs and multiple comparisons. It's more robust than Tukey's HSD but also less powerful. It's suitable for situations where you have a larger number of groups and you're comparing various combinations of group means.\n",
    "\n",
    "    Dunn's Test (Non-parametric):\n",
    "    Dunn's test is a non-parametric alternative to Tukey's HSD. It's used when the assumption of normality is violated or when dealing with ordinal data. This test employs rank-based comparisons and adjusts for multiple testing.\n",
    "\n",
    "    Fisher's LSD (Least Significant Difference):\n",
    "    Fisher's LSD is less stringent than Tukey's HSD and is appropriate when the assumption of homogeneity of variances is met. It's useful for exploratory analyses but may have higher Type I error rates compared to other tests.\n",
    "\n",
    "    Games-Howell Test:\n",
    "    Games-Howell is suitable when the assumption of homogeneity of variances is violated. It doesn't assume equal variances and is more robust in such situations. It's commonly used when dealing with unequal sample sizes and variances across groups.\n",
    "\n",
    "    Holm-Bonferroni Method:\n",
    "    The Holm-Bonferroni method is a stepwise approach that adjusts p-values for multiple comparisons. It's less conservative than the traditional Bonferroni correction while still controlling the familywise error rate.\n",
    "\n",
    "Example situation:\n",
    "Suppose you are conducting an experiment to compare the effectiveness of three different treatments (A, B, and C) on reducing blood pressure. After conducting an ANOVA, you find a significant difference among the group means. To determine which specific treatments differ significantly from each other, you decide to perform post-hoc tests.\n",
    "\n",
    "In this scenario, you might use Tukey's HSD or Bonferroni correction to perform pairwise comparisons and identify which treatments have significantly different effects on blood pressure. If the assumption of normality is violated or if the data is ordinal, you could consider using Dunn's test. If the assumption of equal variances is not met, the Games-Howell test could be appropriate. The choice of post-hoc test depends on the specific characteristics of your data and the assumptions you can reasonably make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc00e949-b2b6-4361-8c6d-40d2c4421ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Statistic: 8.914168610576342\n",
      "P-Value: 0.00022180999236284595\n",
      "Reject the null hypothesis. There is a significant difference between the mean weight loss of the diets.\n"
     ]
    }
   ],
   "source": [
    "#Q9.\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Generate sample data for weight loss in each diet (A, B, C)\n",
    "np.random.seed(42)  # for reproducibility\n",
    "diet_A = np.random.normal(loc=5, scale=2, size=50)  # Mean weight loss: 5 kg, SD: 2 kg\n",
    "diet_B = np.random.normal(loc=4.5, scale=2, size=50)  # Mean weight loss: 4.5 kg, SD: 2 kg\n",
    "diet_C = np.random.normal(loc=6, scale=2, size=50)  # Mean weight loss: 6 kg, SD: 2 kg\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. There is a significant difference between the mean weight loss of the diets.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference between the mean weight loss of the diets.\")\n",
    "    \n",
    "#Interpretation:\n",
    "\n",
    "    #The F-statistic measures the ratio of the between-group variance to the within-group variance. A larger F-statistic indicates a larger difference between the group means.\n",
    "    #The p-value assesses the probability of observing the obtained F-statistic (or a more extreme value) if the null hypothesis were true (i.e., all diet means are equal).\n",
    "\n",
    "#If the p-value is less than the chosen significance level (alpha, typically 0.05), you would reject the null hypothesis. This means that there is a significant difference between the mean weight loss of at least one pair of diets. On the other hand, if the p-value is greater than or equal to alpha, you would fail to reject the null hypothesis, indicating that there is no significant difference between the mean weight loss of the diets.\n",
    "\n",
    "#Remember that the example data provided in this code is for illustrative purposes. You should replace it with your actual collected data to obtain meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80008c4-68c8-43c5-8531-a1243e660ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Main Effect:\n",
      "F-statistic: 0.14\n",
      "P-value: 0.8727\n",
      "Fail to reject null hypothesis: No significant main effect of software programs.\n",
      "\n",
      "Experience Main Effect:\n",
      "F-statistic: 0.14\n",
      "P-value: 0.7134\n",
      "Fail to reject null hypothesis: No significant main effect of employee experience level.\n",
      "\n",
      "Interaction Effect:\n",
      "F-statistic: 0.36\n",
      "P-value: 0.7047\n",
      "Fail to reject null hypothesis: No significant interaction effect.\n"
     ]
    }
   ],
   "source": [
    "#Q10.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "# Simulated data (replace this with your actual data)\n",
    "np.random.seed(42)\n",
    "n = 30\n",
    "software = np.random.choice(['Program A', 'Program B', 'Program C'], size=n)\n",
    "experience = np.random.choice(['Novice', 'Experienced'], size=n)\n",
    "completion_time = np.random.normal(loc=10, scale=2, size=n)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'Software': software, 'Experience': experience, 'CompletionTime': completion_time})\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "formula = 'CompletionTime ~ C(Software) + C(Experience) + C(Software):C(Experience)'\n",
    "model = ols(formula, data).fit()\n",
    "anova_table = anova_lm(model, typ=2)\n",
    "\n",
    "# Extract F-statistics and p-values\n",
    "f_software = anova_table['F'][0]\n",
    "p_software = anova_table['PR(>F)'][0]\n",
    "f_experience = anova_table['F'][1]\n",
    "p_experience = anova_table['PR(>F)'][1]\n",
    "f_interaction = anova_table['F'][2]\n",
    "p_interaction = anova_table['PR(>F)'][2]\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "\n",
    "print(\"Software Main Effect:\")\n",
    "print(f\"F-statistic: {f_software:.2f}\")\n",
    "print(f\"P-value: {p_software:.4f}\")\n",
    "if p_software < alpha:\n",
    "    print(\"Reject null hypothesis: There is a significant main effect of software programs.\")\n",
    "else:\n",
    "    print(\"Fail to reject null hypothesis: No significant main effect of software programs.\")\n",
    "\n",
    "print(\"\\nExperience Main Effect:\")\n",
    "print(f\"F-statistic: {f_experience:.2f}\")\n",
    "print(f\"P-value: {p_experience:.4f}\")\n",
    "if p_experience < alpha:\n",
    "    print(\"Reject null hypothesis: There is a significant main effect of employee experience level.\")\n",
    "else:\n",
    "    print(\"Fail to reject null hypothesis: No significant main effect of employee experience level.\")\n",
    "\n",
    "print(\"\\nInteraction Effect:\")\n",
    "print(f\"F-statistic: {f_interaction:.2f}\")\n",
    "print(f\"P-value: {p_interaction:.4f}\")\n",
    "if p_interaction < alpha:\n",
    "    print(\"Reject null hypothesis: There is a significant interaction effect between software programs and experience.\")\n",
    "else:\n",
    "    print(\"Fail to reject null hypothesis: No significant interaction effect.\")\n",
    "                            \n",
    "#Replace the simulated data with your actual data in the data DataFrame. The code will perform a two-way ANOVA, extract the F-statistics and p-values for main effects and interaction effects, and then interpret the results based on a significance level of 0.05 (adjust as needed).\n",
    "\n",
    "#Remember that this example assumes that the data meets the assumptions of ANOVA, including normality of residuals and homogeneity of variances. If your actual data violates these assumptions, you might need to consider transformations or other statistical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b45d0-f68d-4fcc-b2ea-fd79783dd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q11.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "from statsmodels.stats.libqsturng import psturng\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Test scores data for control and experimental groups\n",
    "control_group_scores = [/* your control group scores */]\n",
    "experimental_group_scores = [/* your experimental group scores */]\n",
    "\n",
    "# Perform a two-sample t-test\n",
    "t_statistic, p_value = ttest_ind(control_group_scores, experimental_group_scores)\n",
    "\n",
    "# Check the p-value to determine if the results are significant\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"The results are statistically significant.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the groups.\")\n",
    "    \n",
    "# Combine the data and create a group indicator\n",
    "all_scores = control_group_scores + experimental_group_scores\n",
    "group_labels = ['Control'] * len(control_group_scores) + ['Experimental'] * len(experimental_group_scores)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'scores': all_scores, 'group': group_labels})\n",
    "\n",
    "# Perform Tukey's HSD test\n",
    "mc = MultiComparison(data['scores'], data['group'])\n",
    "result = mc.tukeyhsd()\n",
    "\n",
    "# Print the results\n",
    "print(result)\n",
    "\n",
    "# Plot the results (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "result.plot_simultaneous(comparison_name=\"Experimental\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8812c8e-3e20-4bfd-b13d-5193f69a8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q12.\n",
    "\n",
    "#    Import the required libraries:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "#    Create a DataFrame with your data. Let's assume you have your sales data in a CSV file named \"sales_data.csv\" with columns \"Store\", \"Day\", and \"Sales\":\n",
    "\n",
    "\n",
    "# Load data from CSV\n",
    "data = pd.read_csv(\"sales_data.csv\")\n",
    "\n",
    "#    Perform a repeated measures ANOVA:\n",
    "\n",
    "\n",
    "\n",
    "# Fit the repeated measures ANOVA model\n",
    "rm_anova_model = ols('Sales ~ C(Store, Sum)*C(Day, Sum)', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(rm_anova_model, typ=2)\n",
    "print(anova_table)\n",
    "\n",
    "#The above code will print an ANOVA table containing information about the main effects and interactions.\n",
    "\n",
    "#    If the ANOVA results are significant, you can proceed with a post-hoc test, like the Tukey's Honestly Significant Difference (HSD) test, to determine which store(s) differ significantly from each other:\n",
    "\n",
    "\n",
    "# Perform Tukey's HSD post-hoc test\n",
    "posthoc = pairwise_tukeyhsd(data['Sales'], data['Store'], alpha=0.05)\n",
    "print(posthoc)\n",
    "\n",
    "#This code will print the post-hoc test results, including confidence intervals and p-values for pairwise comparisons between the stores.\n",
    "\n",
    "#Remember to adjust the significance level (alpha) according to your desired level of significance for both the ANOVA and post-hoc test.\n",
    "\n",
    "#Please adapt the code to match your actual data structure and file format. Additionally, ensure that your data meets the assumptions of ANOVA, such as normality and homogeneity of variances. If your data violates these assumptions, you might need to consider alternative methods or transformations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
